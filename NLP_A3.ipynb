{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jananimylswamy/designsprint-janani/blob/main/NLP_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ],
      "metadata": {
        "id": "32yCsRUo8H33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ],
      "metadata": {
        "id": "XCybYoGz8YWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ],
      "metadata": {
        "id": "6po98qVA8bJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train-claims.json\", \"r\") as f:\n",
        "    train_claims = json.load(f)\n",
        "\n",
        "with open(\"evidence.json\", \"r\") as f:\n",
        "    evidence_corpus = json.load(f)\n"
      ],
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the data\n",
        "with open(\"train-claims.json\", \"r\") as f:\n",
        "    train_claims = json.load(f)\n",
        "\n",
        "with open(\"evidence.json\", \"r\") as f:\n",
        "    evidence_corpus = json.load(f)\n",
        "\n",
        "# Preview the first 5 entries\n",
        "for i, (claim_id, claim_data) in enumerate(train_claims.items()):\n",
        "    print(f\"üîπ Claim ID: {claim_id}\")\n",
        "    print(f\"   ‚úèÔ∏è Claim Text: {claim_data['claim_text']}\")\n",
        "    print(f\"   üè∑Ô∏è Label: {claim_data['claim_label']}\")\n",
        "    print(f\"   üìÑ Evidence IDs: {claim_data['evidences']}\")\n",
        "\n",
        "    # Show the evidence texts\n",
        "    for eid in claim_data[\"evidences\"]:\n",
        "        evidence_text = evidence_corpus.get(eid, \"[MISSING]\")\n",
        "        print(f\"      ‚û§ {eid}: {evidence_text}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    if i == 4:\n",
        "        break  # Show only first 5 claims\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5A2FRtOhJHt",
        "outputId": "a81ee53a-18b3-4e51-8793-afda9ea740bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Claim ID: claim-1937\n",
            "   ‚úèÔ∏è Claim Text: Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
            "   üè∑Ô∏è Label: DISPUTED\n",
            "   üìÑ Evidence IDs: ['evidence-442946', 'evidence-1194317', 'evidence-12171']\n",
            "      ‚û§ evidence-442946: At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.\n",
            "      ‚û§ evidence-1194317: Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.\n",
            "      ‚û§ evidence-12171: Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Claim ID: claim-126\n",
            "   ‚úèÔ∏è Claim Text: El Ni√±o drove record highs in global temperatures suggesting rise may not be down to man-made emissions.\n",
            "   üè∑Ô∏è Label: REFUTES\n",
            "   üìÑ Evidence IDs: ['evidence-338219', 'evidence-1127398']\n",
            "      ‚û§ evidence-338219: While ‚Äòclimate change‚Äô can be due to natural forces or human activity, there is now substantial evidence to indicate that human activity ‚Äì and specifically increased greenhouse gas (GHGs) emissions ‚Äì is a key factor in the pace and extent of global temperature increases.\n",
            "      ‚û§ evidence-1127398: This acceleration is due mostly to human-caused global warming, which is driving thermal expansion of seawater and the melting of land-based ice sheets and glaciers.\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Claim ID: claim-2510\n",
            "   ‚úèÔ∏è Claim Text: In 1946, PDO switched to a cool phase.\n",
            "   üè∑Ô∏è Label: SUPPORTS\n",
            "   üìÑ Evidence IDs: ['evidence-530063', 'evidence-984887']\n",
            "      ‚û§ evidence-530063: There is evidence of reversals in the prevailing polarity (meaning changes in cool surface waters versus warm surface waters within the region) of the oscillation occurring around 1925, 1947, and 1977; the last two reversals corresponded with dramatic shifts in salmon production regimes in the North Pacific Ocean.\n",
            "      ‚û§ evidence-984887: 1945/1946: The PDO changed to a \"cool\" phase, the pattern of this regime shift is similar to the 1970s episode with maximum amplitude in the subarctic and subtropical front but with a greater signature near the Japan while the 1970s shift was stronger near the American west coast.\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Claim ID: claim-2021\n",
            "   ‚úèÔ∏è Claim Text: Weather Channel co-founder John Coleman provided evidence that convincingly refutes the concept of anthropogenic global warming.\n",
            "   üè∑Ô∏è Label: DISPUTED\n",
            "   üìÑ Evidence IDs: ['evidence-1177431', 'evidence-782448', 'evidence-540069', 'evidence-352655', 'evidence-1007867']\n",
            "      ‚û§ evidence-1177431: There is no convincing scientific evidence that human release of carbon dioxide, methane, or other greenhouse gases is causing or will, in the foreseeable future, cause catastrophic heating of the Earth's atmosphere and disruption of the Earth's climate.\n",
            "      ‚û§ evidence-782448: He has called global warming the \"greatest scam in history\" and made numerous false or misleading claims about climate science.\n",
            "      ‚û§ evidence-540069: International Council of Academies of Engineering and Technological Sciences (CAETS) in 2007, issued a Statement on Environment and Sustainable Growth: As reported by the Intergovernmental Panel on Climate Change (IPCC), most of the observed global warming since the mid-20th century is very likely due to human-produced emission of greenhouse gases and this warming will continue unabated if present anthropogenic emissions continue or, worse, expand without control.\n",
            "      ‚û§ evidence-352655: \"Climate Scientists Virtually Unanimous Anthropogenic Global Warming Is True\".\n",
            "      ‚û§ evidence-1007867: Scientists Reach 100% Consensus on Anthropogenic Global Warming.\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Claim ID: claim-2449\n",
            "   ‚úèÔ∏è Claim Text: \"January 2008 capped a 12 month period of global temperature drops on all of the major well respected indicators.\n",
            "   üè∑Ô∏è Label: NOT_ENOUGH_INFO\n",
            "   üìÑ Evidence IDs: ['evidence-1010750', 'evidence-91661', 'evidence-722725', 'evidence-554161', 'evidence-430839']\n",
            "      ‚û§ evidence-1010750: With average temperature +8.1¬†¬∞C (47¬†¬∞F).\n",
            "      ‚û§ evidence-91661: The Iranian / Persian calendar, currently used in Iran and Afghanistan, also has 12 months.\n",
            "      ‚û§ evidence-722725: All of these events can have wide variations of more than a month from year to year.\n",
            "      ‚û§ evidence-554161: Its average duration is 365.256363004 days (365 d 6 h 9 min 9.76 s) (at the epoch J2000.0 = January 1, 2000, 12:00:00 TT).\n",
            "      ‚û§ evidence-430839: It has a duration of approximately 354.37 days.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk --upgrade --force-reinstall\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text, remove_stopwords=False):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and non-alphanumeric characters\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lemmatize and filter\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalnum():  # keep only alphanumeric\n",
        "            if len(token) > 1:  # remove single characters\n",
        "                lemma = lemmatizer.lemmatize(token)\n",
        "                if not remove_stopwords or lemma not in stop_words:\n",
        "                    cleaned_tokens.append(lemma)\n",
        "\n",
        "    return \" \".join(cleaned_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "Hffvn0QLe8hF",
        "outputId": "b8d71a19-eca8-4d50-e098-5ba749f7a9fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "Successfully installed click-8.1.8 joblib-1.5.0 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "nltk",
                  "regex"
                ]
              },
              "id": "cca85fe96fd9409fa1d9158a9f86dbf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "for eid in evidence_corpus:\n",
        "    evidence_corpus[eid] = clean_text(evidence_corpus[eid], remove_stopwords=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUMrvzbmfLk4",
        "outputId": "a0391931-39ad-48b8-afb3-748688b4a34d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_pairs = []\n",
        "\n",
        "for cid, cdata in train_claims.items():\n",
        "    claim = clean_text(cdata[\"claim_text\"], remove_stopwords=False)\n",
        "    for eid in cdata[\"evidences\"]:\n",
        "        if eid in evidence_corpus:\n",
        "            evidence = evidence_corpus[eid]\n",
        "            if len(evidence.split()) > 5:  # filter very short texts\n",
        "                positive_pairs.append((claim, evidence, 1))\n"
      ],
      "metadata": {
        "id": "bK4yRSPRgn1V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "negative_pairs = []\n",
        "all_eids = set(evidence_corpus.keys())\n",
        "\n",
        "for cid, cdata in train_claims.items():\n",
        "    claim = clean_text(cdata[\"claim_text\"], remove_stopwords=False)\n",
        "    gold_eids = set(cdata[\"evidences\"])\n",
        "    candidate_neg_eids = list(all_eids - gold_eids)\n",
        "    sampled_neg_eids = random.sample(candidate_neg_eids, k=3)  # 3 negatives per claim\n",
        "\n",
        "    for eid in sampled_neg_eids:\n",
        "        evidence = evidence_corpus[eid]\n",
        "        if len(evidence.split()) > 5:\n",
        "            negative_pairs.append((claim, evidence, 0))\n"
      ],
      "metadata": {
        "id": "rl7pPLFskAYv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = positive_pairs + negative_pairs\n",
        "random.shuffle(dataset)\n"
      ],
      "metadata": {
        "id": "2rkJQIR2kDFk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(f\"Claim: {dataset[i][0]}\")\n",
        "    print(f\"Evidence: {dataset[i][1]}\")\n",
        "    print(f\"Label: {dataset[i][2]}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5era0yZkEiu",
        "outputId": "b8390086-98c5-4895-ab79-7fb76c41265a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim: during the recent global warming summit in copenhagen nancy pelosi and others stayed at five star hotel on trip costing nearly 10 000 per person\n",
            "Evidence: the document wa subtitled a the copenhagen agreement and proposes measure to keep average global temperature rise to two degree celsius above pre industrial level\n",
            "Label: 1\n",
            "---\n",
            "Claim: and since the last ice age ended almost exactly 11 500 year ago ice age now\n",
            "Evidence: the last cold episode of the last glacial period ended about 10 000 year ago\n",
            "Label: 1\n",
            "---\n",
            "Claim: the modeler confused cause and effect thereby getting the feedback in the wrong direction\n",
            "Evidence: the episode begin with the planet express ship crash landing on earth killing many of the main character\n",
            "Label: 0\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ],
      "metadata": {
        "id": "1FA2ao2l8hOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Build a Vocabulary\n",
        "You need to turn words into numbers (token IDs).\n"
      ],
      "metadata": {
        "id": "9_VuMA_zsDvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build vocab from all claim + evidence tokens\n",
        "counter = Counter()\n",
        "\n",
        "for claim, evidence, _ in dataset:\n",
        "    counter.update(claim.split())\n",
        "    counter.update(evidence.split())\n",
        "\n",
        "# Special tokens\n",
        "vocab = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[UNK]\": 3}\n",
        "for word in counter:\n",
        "    if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n"
      ],
      "metadata": {
        "id": "pRdBWBBJsCD9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Encode Text into Token IDs\n",
        "Create input sequences like:"
      ],
      "metadata": {
        "id": "fSsU22JNsHqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CLS] claim tokens [SEP] evidence tokens [SEP]\n"
      ],
      "metadata": {
        "id": "SQuhJyttsMbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "def encode(claim, evidence, vocab):\n",
        "    tokens = [\"[CLS]\"] + claim.split() + [\"[SEP]\"] + evidence.split() + [\"[SEP]\"]\n",
        "    token_ids = [vocab.get(tok, vocab[\"[UNK]\"]) for tok in tokens]\n",
        "\n",
        "    if len(token_ids) < MAX_LEN:\n",
        "        token_ids += [vocab[\"[PAD]\"]] * (MAX_LEN - len(token_ids))\n",
        "    else:\n",
        "        token_ids = token_ids[:MAX_LEN]\n",
        "\n",
        "    return token_ids\n"
      ],
      "metadata": {
        "id": "WlwLz47asJkV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now encode all your data:\n"
      ],
      "metadata": {
        "id": "iF_V3dzbsPDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "labels = []\n",
        "\n",
        "for claim, evidence, label in dataset:\n",
        "    encoded = encode(claim, evidence, vocab)\n",
        "    input_ids.append(encoded)\n",
        "    labels.append(label)\n"
      ],
      "metadata": {
        "id": "LH48ayODsREv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a PyTorch Dataset\n",
        "This wraps your input data for training:"
      ],
      "metadata": {
        "id": "4fVJoBCusVRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EvidenceDataset(Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.input_ids[idx]), torch.tensor(self.labels[idx], dtype=torch.float)\n"
      ],
      "metadata": {
        "id": "wmOCTzR9sW__"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataset and dataloader:\n"
      ],
      "metadata": {
        "id": "PVtkaGplsamH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = EvidenceDataset(input_ids, labels)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "01_IxcrMsb2-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Define a Simple Transformer Model\n",
        "This model reads your token IDs and outputs a score (0‚Äì1) for each pair:"
      ],
      "metadata": {
        "id": "4UICeUUcsdWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ShallowTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, max_len=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1), :]\n",
        "        x = self.transformer(x)\n",
        "        cls_token = x[:, 0, :]  # Use [CLS] token output\n",
        "        return self.classifier(cls_token).squeeze(-1)  # shape: [batch_size]\n"
      ],
      "metadata": {
        "id": "O9ywB4XCshJ_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Train the Model\n"
      ],
      "metadata": {
        "id": "7lgwTENJsjVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ShallowTransformer(vocab_size=len(vocab))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "HotOIspfslc4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_inputs)\n",
        "        loss = criterion(predictions, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2gwBktWsl5R",
        "outputId": "999e8e99-1dc5-423f-a191-75b8e42169f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6516\n",
            "Epoch 2, Loss: 0.5681\n",
            "Epoch 3, Loss: 0.4878\n",
            "Epoch 4, Loss: 0.4305\n",
            "Epoch 5, Loss: 0.3886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model Performance\n"
      ],
      "metadata": {
        "id": "Tps9y3bvyuy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in train_loader:  # use validation/test data\n",
        "        outputs = model(input_ids)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Accuracy: {correct / total:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY8eOhwQyuPJ",
        "outputId": "51f7796c-40b4-4654-b5aa-9de316a140d0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 86.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Set"
      ],
      "metadata": {
        "id": "UdJsAP8v0dc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load dev-claims.json\n",
        "with open(\"dev-claims.json\") as f:\n",
        "    dev_claims = json.load(f)\n",
        "\n",
        "# Step 2: Create positive/negative (claim, evidence, label) pairs\n",
        "val_dataset = []\n",
        "for cid, data in dev_claims.items():\n",
        "    claim_text = clean_text(data[\"claim_text\"])\n",
        "    gold_evidence_ids = set(data[\"evidences\"])\n",
        "\n",
        "    # Positive examples\n",
        "    for eid in gold_evidence_ids:\n",
        "        if eid in evidence_corpus:\n",
        "            evidence = clean_text(evidence_corpus[eid])\n",
        "            val_dataset.append((claim_text, evidence, 1))\n",
        "\n",
        "    # Negative examples (sample a few that are not gold)\n",
        "    negative_ids = list(set(evidence_corpus.keys()) - gold_evidence_ids)\n",
        "    sampled = random.sample(negative_ids, 3)\n",
        "    for eid in sampled:\n",
        "        evidence = clean_text(evidence_corpus[eid])\n",
        "        val_dataset.append((claim_text, evidence, 0))\n",
        "\n",
        "# Step 3: Encode\n",
        "val_input_ids = [encode(c, e, vocab) for c, e, _ in val_dataset]\n",
        "val_labels = [lbl for _, _, lbl in val_dataset]\n",
        "\n",
        "# Step 4: Wrap into DataLoader\n",
        "val_loader = DataLoader(EvidenceDataset(val_input_ids, val_labels), batch_size=32)\n"
      ],
      "metadata": {
        "id": "FvxvnhJ50cNz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in val_loader:  # ‚úÖ use val_loader here\n",
        "        outputs = model(input_ids)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {correct / total:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpsuM5aE1M_g",
        "outputId": "3df91033-1a14-47fc-de2c-63c01d727347"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 80.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"shallow_transformer.pth\")"
      ],
      "metadata": {
        "id": "LgI8CEovzHCl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import heapq  # for efficient top-k\n",
        "\n",
        "def retrieve_top_k_evidence(claim, evidence_corpus, model, vocab, k=5, max_len=128):\n",
        "    model.eval()\n",
        "    scored_passages = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for eid, evidence in evidence_corpus.items():\n",
        "            input_ids = encode(claim, evidence, vocab)\n",
        "            input_tensor = torch.tensor([input_ids])  # batch size = 1\n",
        "\n",
        "            score = model(input_tensor).item()  # relevance score (0 to 1)\n",
        "            scored_passages.append((score, eid, evidence))\n",
        "\n",
        "    # Sort by score descending and return top-k\n",
        "    top_k = heapq.nlargest(k, scored_passages, key=lambda x: x[0])\n",
        "\n",
        "    return top_k\n"
      ],
      "metadata": {
        "id": "GfvtMUHBy_Ff"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ],
      "metadata": {
        "id": "EzGuzHPE87Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ],
      "metadata": {
        "id": "mefSOe8eTmGP"
      }
    }
  ]
}